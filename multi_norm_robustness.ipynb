{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1BYApFgzteA",
        "outputId": "9ff4eb24-91f8-4c95-f9ba-17cccfb47b8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:03<00:00, 49.1MB/s]\n"
          ]
        }
      ],
      "source": [
        "# !pip install tensorboardX\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "# from tensorboardX import SummaryWriter\n",
        "\n",
        "use_cuda = True\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print(device)\n",
        "batch_size = 64\n",
        "\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "\n",
        "## Dataloaders\n",
        "# train_dataset = datasets.CIFAR10('cifar10_data/', train=True, download=True, transform=transforms.Compose(\n",
        "#     [transforms.ToTensor()]\n",
        "# ))\n",
        "test_dataset = datasets.CIFAR10('cifar10_data/', train=False, download=True, transform=transforms.Compose(\n",
        "    [transforms.ToTensor()]\n",
        "))\n",
        "\n",
        "# train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def tp_relu(x, delta=1.):\n",
        "    ind1 = (x < -1. * delta).float()\n",
        "    ind2 = (x > delta).float()\n",
        "    return .5 * (x + delta) * (1 - ind1) * (1 - ind2) + x * ind2\n",
        "\n",
        "def tp_smoothed_relu(x, delta=1.):\n",
        "    ind1 = (x < -1. * delta).float()\n",
        "    ind2 = (x > delta).float()\n",
        "    return (x + delta) ** 2 / (4 * delta) * (1 - ind1) * (1 - ind2) + x * ind2\n",
        "\n",
        "class Normalize(nn.Module):\n",
        "    def __init__(self, mu, std):\n",
        "        super(Normalize, self).__init__()\n",
        "        self.mu, self.std = mu, std\n",
        "\n",
        "    def forward(self, x):\n",
        "        return (x - self.mu) / self.std\n",
        "\n",
        "class IdentityLayer(nn.Module):\n",
        "    def forward(self, inputs):\n",
        "        return inputs\n",
        "\n",
        "class PreActBlock(nn.Module):\n",
        "    '''Pre-activation version of the BasicBlock.'''\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, bn, learnable_bn, stride=1, activation='relu'):\n",
        "        super(PreActBlock, self).__init__()\n",
        "        self.collect_preact = True\n",
        "        self.activation = activation\n",
        "        self.avg_preacts = []\n",
        "        self.bn1 = nn.BatchNorm2d(in_planes, affine=learnable_bn) if bn else IdentityLayer()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=not learnable_bn)\n",
        "        self.bn2 = nn.BatchNorm2d(planes, affine=learnable_bn) if bn else IdentityLayer()\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=not learnable_bn)\n",
        "\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=not learnable_bn)\n",
        "            )\n",
        "\n",
        "    def act_function(self, preact):\n",
        "        if self.activation == 'relu':\n",
        "            act = F.relu(preact)\n",
        "        elif self.activation[:6] == '3prelu':\n",
        "            act = tp_relu(preact, delta=float(self.activation.split('relu')[1]))\n",
        "        elif self.activation[:8] == '3psmooth':\n",
        "            act = tp_smoothed_relu(preact, delta=float(self.activation.split('smooth')[1]))\n",
        "        else:\n",
        "            assert self.activation[:8] == 'softplus'\n",
        "            beta = int(self.activation.split('softplus')[1])\n",
        "            act = F.softplus(preact, beta=beta)\n",
        "        return act\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.act_function(self.bn1(x))\n",
        "        shortcut = self.shortcut(out) if hasattr(self, 'shortcut') else x  # Important: using out instead of x\n",
        "        out = self.conv1(out)\n",
        "        out = self.conv2(self.act_function(self.bn2(out)))\n",
        "        out += shortcut\n",
        "        return out\n",
        "\n",
        "class PreActResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, n_cls, cuda=True, half_prec=False,\n",
        "        activation='relu', fts_before_bn=False, normal='none'):\n",
        "        super(PreActResNet, self).__init__()\n",
        "        self.bn = True\n",
        "        self.learnable_bn = True  # doesn't matter if self.bn=False\n",
        "        self.in_planes = 64\n",
        "        self.avg_preact = None\n",
        "        self.activation = activation\n",
        "        self.fts_before_bn = fts_before_bn\n",
        "        if normal == 'cifar10':\n",
        "            self.mu = torch.tensor((0.4914, 0.4822, 0.4465)).view(1, 3, 1, 1)\n",
        "            self.std = torch.tensor((0.2471, 0.2435, 0.2616)).view(1, 3, 1, 1)\n",
        "        else:\n",
        "            self.mu = torch.tensor((0.0, 0.0, 0.0)).view(1, 3, 1, 1)\n",
        "            self.std = torch.tensor((1.0, 1.0, 1.0)).view(1, 3, 1, 1)\n",
        "            print('no input normalization')\n",
        "        if cuda:\n",
        "            self.mu = self.mu.cuda()\n",
        "            self.std = self.std.cuda()\n",
        "        if half_prec:\n",
        "            self.mu = self.mu.half()\n",
        "            self.std = self.std.half()\n",
        "\n",
        "        self.normalize = Normalize(self.mu, self.std)\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=not self.learnable_bn)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.bn = nn.BatchNorm2d(512 * block.expansion)\n",
        "        self.linear = nn.Linear(512*block.expansion, n_cls)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, self.bn, self.learnable_bn, stride, self.activation))\n",
        "            # layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x, return_features=False):\n",
        "        for layer in [*self.layer1, *self.layer2, *self.layer3, *self.layer4]:\n",
        "            layer.avg_preacts = []\n",
        "\n",
        "        out = self.normalize(x)\n",
        "        out = self.conv1(out)\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        if return_features and self.fts_before_bn:\n",
        "            return out.view(out.size(0), -1)\n",
        "        out = F.relu(self.bn(out))\n",
        "        if return_features:\n",
        "            return out.view(out.size(0), -1)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "def PreActResNet18(n_cls, cuda=True, half_prec=False, activation='relu', fts_before_bn=False,\n",
        "    normal='none'):\n",
        "    #print('initializing PA RN-18 with act {}, normal {}'.format())\n",
        "    return PreActResNet(PreActBlock, [2, 2, 2, 2], n_cls=n_cls, cuda=cuda, half_prec=half_prec,\n",
        "        activation=activation, fts_before_bn=fts_before_bn, normal=normal)\n",
        "\n",
        "\n",
        "# intialize the model\n",
        "model = PreActResNet18(10, cuda=True, activation='softplus1').to(device)\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPQ3cWuc0KTF",
        "outputId": "12cf74a8-4742-4de5-da04-5f786d98892f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no input normalization\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PreActResNet(\n",
              "  (normalize): Normalize()\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): PreActBlock(\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    )\n",
              "    (1): PreActBlock(\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): PreActBlock(\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (shortcut): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (1): PreActBlock(\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): PreActBlock(\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (shortcut): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (1): PreActBlock(\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): PreActBlock(\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (shortcut): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (1): PreActBlock(\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    )\n",
              "  )\n",
              "  (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (linear): Linear(in_features=512, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def pgd_linf_untargeted(model, x, labels, k, eps, eps_step):\n",
        "    model.eval()\n",
        "    ce_loss = torch.nn.CrossEntropyLoss()\n",
        "    adv_x = x.clone().detach()\n",
        "    adv_x.requires_grad_(True)\n",
        "    for _ in range(k):\n",
        "        adv_x.requires_grad_(True)\n",
        "        model.zero_grad()\n",
        "        output = model(adv_x)\n",
        "        # TODO: Calculate the loss\n",
        "        loss = ce_loss(output, labels)\n",
        "        loss.backward()\n",
        "        # TODO: compute the adv_x\n",
        "        adv_x = adv_x + eps_step * adv_x.grad.sign()\n",
        "        # find delta, clamp with eps\n",
        "        delta = torch.clamp(adv_x - x, min=-eps, max=eps)\n",
        "        adv_x = torch.clamp(x + delta, min=0, max=1).detach()\n",
        "    return adv_x"
      ],
      "metadata": {
        "id": "YNCFKW6g0zxh"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pgd_l2_untargeted(model, x, labels, k, eps, eps_step):\n",
        "    model.eval()\n",
        "    ce_loss = torch.nn.CrossEntropyLoss()\n",
        "    adv_x = x.clone().detach()\n",
        "    adv_x.requires_grad_(True)\n",
        "    for _ in range(k):\n",
        "        adv_x.requires_grad_(True)\n",
        "        model.zero_grad()\n",
        "        output = model(adv_x)\n",
        "        batch_size = x.size()[0]\n",
        "        # TODO: Calculate the loss\n",
        "        loss = ce_loss(output, labels)\n",
        "        loss.backward()\n",
        "        grad = adv_x.grad.data\n",
        "        # TODO: compute the adv_x\n",
        "        # find delta, clamp with eps, project delta to the l2 ball\n",
        "        # HINT: https://github.com/Harry24k/adversarial-attacks-pytorch/blob/master/torchattacks/attacks/pgdl2.py\n",
        "\n",
        "        small_delta = 1e-10\n",
        "        # update\n",
        "        adv_x = adv_x + eps_step * grad / (small_delta + grad.norm(p=2))\n",
        "        # project onto L2 ball\n",
        "        delta = adv_x - x\n",
        "        delta_norm = delta.view(delta.size(0), -1).norm(p=2, dim=1, keepdim=True)\n",
        "        factor = torch.max(delta_norm, torch.tensor(eps, device=delta_norm.device))\n",
        "        factor = factor.view(-1,1,1,1)\n",
        "        delta_proj = delta * (eps / factor)\n",
        "        adv_x = x + delta_proj\n",
        "        # clamp\n",
        "        adv_x = torch.clamp(adv_x, min=0, max=1).detach()\n",
        "    return adv_x"
      ],
      "metadata": {
        "id": "jRkGJXYP02Ce"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model_on_single_attack(model, attack='pgd_linf', eps=0.1):\n",
        "    model.eval()\n",
        "    tot_test, tot_acc = 0.0, 0.0\n",
        "    tot_acc_clean = 0.0 # for standard accuracy\n",
        "    for batch_idx, (x_batch, y_batch) in tqdm(enumerate(test_loader), total=len(test_loader), desc=\"Evaluating\"):\n",
        "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "        if attack == 'pgd_linf':\n",
        "            # TODO: get x_adv untargeted pgd linf with eps, and eps_step=eps/4\n",
        "            x_adv = pgd_linf_untargeted(model, x_batch, y_batch, 10, eps, eps/4)\n",
        "        elif attack == 'pgd_l2':\n",
        "            # TODO: get x_adv untargeted pgd l2 with eps, and eps_step=eps/4\n",
        "            x_adv = pgd_l2_untargeted(model, x_batch, y_batch, 10, eps, eps/4)\n",
        "        else:\n",
        "            x_adv = x_batch\n",
        "\n",
        "        # get the testing accuracy and update tot_test and tot_acc\n",
        "        # robust accuracy\n",
        "        with torch.no_grad():\n",
        "          logits_adv = model(x_adv)\n",
        "          pred_adv = logits_adv.argmax(dim=1)\n",
        "          tot_acc += (pred_adv == y_batch).sum().item()\n",
        "        # standard accuracy\n",
        "        with torch.no_grad():\n",
        "          logits_clean = model(x_batch)\n",
        "          pred_clean = logits_clean.argmax(dim=1)\n",
        "          tot_acc_clean += (pred_clean == y_batch).sum().item()\n",
        "\n",
        "        tot_test += x_batch.shape[0]\n",
        "\n",
        "    print('Standard accuracy %.51f' % (tot_acc_clean / tot_test))\n",
        "    print('Robust accuracy %.5lf' % (tot_acc/tot_test), f'on {attack} attack with eps = {eps}')"
      ],
      "metadata": {
        "id": "nOrSxijn05aC"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate on Linf attack with different models with eps = 8/255\n",
        "\n",
        "# Evaluate on Linf attack with model 1 with eps = 8/255\n",
        "model.load_state_dict(torch.load('models/pretr_Linf.pth'))\n",
        "test_model_on_single_attack(model, attack='pgd_linf', eps=8/255)\n",
        "\n",
        "# Evaluate on Linf attack with model 2 with eps = 8/255\n",
        "model.load_state_dict(torch.load('models/pretr_L2.pth'))\n",
        "test_model_on_single_attack(model, attack='pgd_linf', eps=8/255)\n",
        "\n",
        "# Evaluate on Linf attack with model 3 with eps = 8/255\n",
        "model.load_state_dict(torch.load('models/pretr_RAMP.pth'))\n",
        "test_model_on_single_attack(model, attack='pgd_linf', eps=8/255)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "blE6ZM6a1EqY",
        "outputId": "e5fc1383-4284-468f-e3cd-c860ab836554"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 157/157 [01:22<00:00,  1.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Standard accuracy 0.827999999999999958255614274094114080071449279785156\n",
            "Robust accuracy 0.51210 on pgd_linf attack with eps = 0.03137254901960784\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 157/157 [01:21<00:00,  1.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Standard accuracy 0.887499999999999955591079014993738383054733276367188\n",
            "Robust accuracy 0.30850 on pgd_linf attack with eps = 0.03137254901960784\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 157/157 [01:21<00:00,  1.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Standard accuracy 0.811899999999999955058171963173663243651390075683594\n",
            "Robust accuracy 0.49770 on pgd_linf attack with eps = 0.03137254901960784\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate on L2 attack with different models with eps = 0.75\n",
        "model.load_state_dict(torch.load('models/pretr_Linf.pth'))\n",
        "# Evaluate on L2 attack with model 1 with eps = 0.75\n",
        "test_model_on_single_attack(model, attack='pgd_l2', eps=0.75)\n",
        "model.load_state_dict(torch.load('models/pretr_L2.pth'))\n",
        "# Evaluate on L2 attack with model 2 with eps = 0.75\n",
        "test_model_on_single_attack(model, attack='pgd_l2', eps=0.75)\n",
        "model.load_state_dict(torch.load('models/pretr_RAMP.pth'))\n",
        "# Evaluate on L2 attack with model 3 with eps = 0.75\n",
        "test_model_on_single_attack(model, attack='pgd_l2', eps=0.75)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xPa9Lxac1IwJ",
        "outputId": "55da9359-6188-416e-876e-5b3704410dfe"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 157/157 [01:22<00:00,  1.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Standard accuracy 0.827999999999999958255614274094114080071449279785156\n",
            "Robust accuracy 0.73900 on pgd_l2 attack with eps = 0.75\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 157/157 [01:22<00:00,  1.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Standard accuracy 0.887499999999999955591079014993738383054733276367188\n",
            "Robust accuracy 0.81600 on pgd_l2 attack with eps = 0.75\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 157/157 [01:22<00:00,  1.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Standard accuracy 0.811899999999999955058171963173663243651390075683594\n",
            "Robust accuracy 0.74830 on pgd_l2 attack with eps = 0.75\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model_on_multi_attacks(model, eps_linf=8./255., eps_l2=0.75):\n",
        "    model.eval()\n",
        "    tot_test, tot_acc = 0.0, 0.0\n",
        "    for batch_idx, (x_batch, y_batch) in tqdm(enumerate(test_loader), total=len(test_loader), desc=\"Evaluating\"):\n",
        "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "        # TODO: get x_adv_linf and x_adv_l2 untargeted pgd linf and l2 with eps, and eps_step=eps/4\n",
        "        x_adv_linf = pgd_linf_untargeted(model, x_batch, y_batch, 10, eps_linf, eps_linf/4)\n",
        "        x_adv_l2 = pgd_l2_untargeted(model, x_batch, y_batch, 10, eps_l2, eps_l2/4)\n",
        "\n",
        "        ## calculate union accuracy: correct only if both attacks are correct\n",
        "\n",
        "        out = model(x_adv_linf)\n",
        "        pred_linf = torch.max(out, dim=1)[1]\n",
        "        out = model(x_adv_l2)\n",
        "        pred_l2 = torch.max(out, dim=1)[1]\n",
        "\n",
        "        # TODO: get the testing accuracy with multi-norm robustness and update tot_test and tot_acc\n",
        "        union_acc = (pred_linf == y_batch) | (pred_l2 == y_batch)\n",
        "        tot_acc += union_acc.sum().item()\n",
        "        tot_test += x_batch.size(0)\n",
        "\n",
        "    print('Robust accuracy %.5lf' % (tot_acc/tot_test), f'on multi attacks')"
      ],
      "metadata": {
        "id": "6lCQEscO1LXG"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate on L2 attack with different models with eps = 0.5\n",
        "model.load_state_dict(torch.load('models/pretr_Linf.pth'))\n",
        "# Evaluate on multi attacks with model 1\n",
        "test_model_on_multi_attacks(model, eps_linf=0.5, eps_l2=0.5)\n",
        "\n",
        "model.load_state_dict(torch.load('models/pretr_L2.pth'))\n",
        "# Evaluate on multi attacks with model 2\n",
        "test_model_on_multi_attacks(model, eps_linf=0.5, eps_l2=0.5)\n",
        "\n",
        "model.load_state_dict(torch.load('models/pretr_RAMP.pth'))\n",
        "# Evaluate on multi attacks with model 3\n",
        "test_model_on_multi_attacks(model, eps_linf=0.5, eps_l2=0.5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4vtCoK61ME1",
        "outputId": "c5b2aad2-84f9-43dd-b481-b2a44af3b06e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 157/157 [02:38<00:00,  1.01s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Robust accuracy 0.76360 on multi attacks\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 157/157 [02:38<00:00,  1.01s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Robust accuracy 0.83030 on multi attacks\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 157/157 [02:38<00:00,  1.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Robust accuracy 0.76610 on multi attacks\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}